{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learn import em_learn, svd_learn_new\n",
    "from data import *\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_learn(sample, n, L=None, verbose=None, stats={}):\n",
    "    Os = np.moveaxis(sample.all_trail_probs(), 1, 0)\n",
    "\n",
    "    svds = [np.linalg.svd(Os[j], full_matrices=True) for j in range(n)]\n",
    "\n",
    "    if verbose:\n",
    "        for i, (_, s, _) in enumerate(svds):\n",
    "            print(f\"{i}: {s[:L+1]} ...\")\n",
    "\n",
    "    Ps_ = np.zeros((n, L, n))\n",
    "    Qs_ = np.zeros((n, L, n))\n",
    "    for j, (u, s, vh) in enumerate(svds):\n",
    "        Ps_[j, 0 : min(n, L), :] = u[:, 0:L].T\n",
    "        Qs_[j, 0 : min(n, L), :] = (np.diag(s) @ (vh))[0:L, :]\n",
    "\n",
    "    A = np.zeros((2 * n * L, n**2))\n",
    "    for j in range(n):\n",
    "        A[L * j : L * (j + 1), n * j : n * (j + 1)] = Ps_[j]\n",
    "        A[L * (n + j) : L * (n + j + 1), j + n * (np.arange(n))] = -Qs_[j]\n",
    "\n",
    "    _, s, vh = np.linalg.svd(A.T, full_matrices=True)\n",
    "    small = list(s < 1e-5)\n",
    "    if True in small:\n",
    "        fst = small.index(True)\n",
    "        if verbose:\n",
    "            print(2 * L * n - fst, L, s[[fst - 1, fst]])\n",
    "    B = vh[-L:]\n",
    "    Bre = np.moveaxis(B.reshape((L, L, 2 * n), order=\"F\"), -1, 0)\n",
    "    Ys_ = Bre[0:n]\n",
    "    Zs_ = Bre[n : 2 * n]\n",
    "\n",
    "    Xs = [\n",
    "        np.linalg.pinv(Zs_[j] @ Ys_[j].T) @ (Zs_[j + 1] @ Ys_[j + 1].T)\n",
    "        for j in range(n - 1)\n",
    "    ]\n",
    "    X = np.sum(Xs, axis=0)\n",
    "    _, R_ = np.linalg.eig(X)\n",
    "    d, _, _, _ = np.linalg.lstsq(\n",
    "        (R_.T @ Ys_[0] @ Ps_[0]).T, Os[0] @ np.ones(n), rcond=None\n",
    "    )\n",
    "\n",
    "    R = np.diag(d) @ R_.T\n",
    "    Ys = R @ Ys_\n",
    "    Ps = np.array([Y @ P_ for Y, P_ in zip(Ys, Ps_)])\n",
    "    Ss = np.array([R @ Z_ @ Y_.T @ R.T for Z_, Y_ in zip(Zs_, Ys_)])\n",
    "\n",
    "    S_ = np.zeros((L, n))\n",
    "    Ms_ = np.zeros((L, n, n))\n",
    "    for l in range(L):\n",
    "        for i in range(n):\n",
    "            S_[l, i] = Ss[i, l, l]\n",
    "            for j in range(n):\n",
    "                Ms_[l, i, j] = Ps[j, l, i] / S_[l, i]\n",
    "\n",
    "    S_ = np.abs(S_)\n",
    "    Ms_ = np.abs(Ms_)\n",
    "    learned_mixture = Mixture(S_, Ms_)\n",
    "    learned_mixture.normalize()\n",
    "    return learned_mixture\n",
    "\n",
    "learners = {\n",
    "    \"CA-SVD\": svd_learn_new,\n",
    "    \"CA-SVD'\": lambda d, n, L: svd_learn_new(d, n, L, sample_dist=0.01),\n",
    "    \"GKV-SVD\": svd_learn,\n",
    "    \"EM2\": lambda d, n, L: em_learn(d, n, L, max_iter=2),\n",
    "    \"EM5\": lambda d, n, L: em_learn(d, n, L, max_iter=5),\n",
    "    \"EM20\": lambda d, n, L: em_learn(d, n, L, max_iter=20),\n",
    "    \"EM50\": lambda d, n, L: em_learn(d, n, L, max_iter=50),\n",
    "    \"EM100\": lambda d, n, L: em_learn(d, n, L, max_iter=100),\n",
    "    \"EM-converge\": em_learn,\n",
    "    \"CA-SVD-EM2\": lambda d, n, L: svd_learn_new(d, n, L, em_refine_max_iter=2),\n",
    "    \"CA-SVD-EM5\": lambda d, n, L: svd_learn_new(d, n, L, em_refine_max_iter=5),\n",
    "    \"CA-SVD-EM20\": lambda d, n, L: svd_learn_new(d, n, L, em_refine_max_iter=20),\n",
    "    \"CA-SVD-EM100\": lambda d, n, L: svd_learn_new(d, n, L, em_refine_max_iter=100),\n",
    "}\n",
    "\n",
    "def count_3_from_seq(seq, n):\n",
    "    \"\"\"\n",
    "    seq: discretized sequence\n",
    "    n: number of categories\n",
    "    \"\"\"\n",
    "    all_trail_probs = np.zeros((n, n, n)) \n",
    "    for i in range(len(seq) // 3):\n",
    "        x = seq[3*i:3*(i+1)]\n",
    "        all_trail_probs[tuple(x)] += 1\n",
    "       #num_visited[x] += 1\n",
    "    return Distribution.from_all_trail_probs(all_trail_probs / np.sum(all_trail_probs))\n",
    "    \n",
    "def learn_mix_from_seq(seq,learner, n, L):\n",
    "    \"\"\"\n",
    "    seq: discretized time series: an 1-d array\n",
    "    learner: \n",
    "    \"\"\"\n",
    "    trail_empirical_distribution = count_3_from_seq(seq, n)\n",
    "    if np.isnan(trail_empirical_distribution.all_trail_probs()).any() or np.isinf(trail_empirical_distribution.all_trail_probs()).any():\n",
    "        print(\"Inf or NAN values\")\n",
    "        print(trail_empirical_distribution.all_trail_probs())\n",
    "        \n",
    "    return  learners[learner](trail_empirical_distribution, n, L)\n",
    "\n",
    "def likelihood(mixture, trails, counts=None, log=False):\n",
    "    if counts is None: counts = transitions(mixture.n, trails)\n",
    "    logS = np.log(mixture.S + 1e-10)\n",
    "    logTs = np.log(mixture.Ms + 1e-10)\n",
    "\n",
    "    logl = logS[:, trails[:,0]]\n",
    "    logl += np.sum(logTs[:, :, :, None] * np.moveaxis(counts, 0, 2)[None, :, :, :], axis=(1,2))\n",
    "    if log: return logl\n",
    "    probs = np.exp(logl - np.max(logl, axis=0))\n",
    "    probs /= np.sum(probs, axis=0)[None, :]\n",
    "    return probs\n",
    "\n",
    "def transitions(n, trails):\n",
    "    n_samples = trails.shape[0]\n",
    "    c = np.zeros([n_samples, n, n], dtype=int)\n",
    "    for t, trail in enumerate(trails):\n",
    "        i = trail[0]\n",
    "        for j in trail[1:]:\n",
    "            c[t, i, j] += 1\n",
    "            i = j\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 10\n",
    "L_chains = 5\n",
    "current_state = 3\n",
    "mix = Mixture.random(n_states, L_chains)\n",
    "ground_truth_distribution = Distribution.from_mixture(mix, t_len=3)\n",
    "sample_distribution = ground_truth_distribution.sample(n_samples = int(1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kx/06y42x8s5571j9jqkpc5wf500000gn/T/ipykernel_31146/2861410156.py:51: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  S_[l, i] = Ss[i, l, l]\n",
      "/var/folders/kx/06y42x8s5571j9jqkpc5wf500000gn/T/ipykernel_31146/2861410156.py:53: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  Ms_[l, i, j] = Ps[j, l, i] / S_[l, i]\n"
     ]
    }
   ],
   "source": [
    "learned_mixture = learners[\"GKV-SVD\"](sample_distribution, n_states, L = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.388065888426134"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_distribution = Distribution.from_mixture(learned_mixture, 3)\n",
    "learned_distribution.dist(ground_truth_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33967892643230074"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_mixture.recovery_error(mix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
